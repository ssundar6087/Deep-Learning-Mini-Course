{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1DfmuC_ClBQlwo5LztUEtFTA391m4JnLp","timestamp":1665206617341},{"file_id":"1b52y2OosIWkdyNVqITs-LKCxjkR_1GHM","timestamp":1665205349714},{"file_id":"19OqQ7FR1IBbmN1lS-BsCwHKdEV0LcVsa","timestamp":1665203799318},{"file_id":"1MeWh78fXU0GbNwbw2uTJySmfvw1KWjKq","timestamp":1665103902452},{"file_id":"1pwiOBgRVAgW7KGVFdJWdTTamlQ7bfUXe","timestamp":1665103747612}],"collapsed_sections":[],"authorship_tag":"ABX9TyNI5pwIqH8cE4nkTrkE+9VQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["<a href=\"https://colab.research.google.com/github/ssundar6087/Deep-Learning-Mini-Course/blob/main/Pytorch/DL_Minicourse_Pytorch_Day_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"],"metadata":{"id":"7kzE2_3Dt3kE"}},{"cell_type":"markdown","source":["# Deconstructing the Model Part 2\n","Today, we'll look at the model. Note that 90% of the code will be the same as the previous notebook. Our focus is to play with the model architecture some more and see if we can improve results."],"metadata":{"id":"e--Ti2Ov8GTm"}},{"cell_type":"markdown","source":["# Image Classification Pytorch"],"metadata":{"id":"tGV5DlzgRFNK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJmKqClAQ4x9"},"outputs":[],"source":["# Imports\n","import torch \n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"markdown","source":["## **YOUR EXERCISE HERE: Improve the Model  ðŸ‘‡** \n","In today's exercise, you'll improve a convolutional neural network. Use your own conv net from the previous exercise if possible. \n","\n","Add the following to the network:\n","- Batch normalization\n","- Dropout\n","- Swap activations (try using Sigmoid, Tanh, Leaky ReLU) instead of the ReLU activation and see what happens\n","\n","**HINT**: https://towardsdatascience.com/batch-normalization-and-dropout-in-neural-networks-explained-with-pytorch-47d7a8459bcd"],"metadata":{"id":"SMBKXZyURnyE"}},{"cell_type":"code","source":["class BabyThanos(nn.Module):\n","  def __init__(self, in_dims, in_channels, n_classes=10):\n","    super().__init__()\n","    \n","    self.in_dims = in_dims \n","    self.in_channels = in_channels\n","    self.n_classes = n_classes\n","    self.k = 5\n","    self.n_filters1 = 6\n","    self.n_filters2 = 16\n","    self.fc1_dim = 100\n","    self.fc2_dim = 40\n","    self.pool_size = 2\n","    self.pool_stride = 2\n","    self.final_dim = self.__compute_flattened_dim__(num_conv_pools=2)\n","\n","    # define the layers here\n","    self.conv1 = nn.Conv2d(self.in_channels, self.n_filters1, self.k)\n","    self.pool = nn.MaxPool2d(self.pool_size, self.pool_stride)\n","    self.conv2 = nn.Conv2d(self.n_filters1, self.n_filters2, self.k)\n","    self.fc1 = nn.Linear(self.n_filters2 * self.final_dim * self.final_dim, self.fc1_dim)\n","    self.fc2 = nn.Linear(self.fc1_dim, self.fc2_dim)\n","    self.fc3 = nn.Linear(self.fc2_dim, self.n_classes)\n","    # define batch norm and dropout layers\n","\n","  def __compute_flattened_dim__(self, num_conv_pools):\n","    final_dim = self.in_dims\n","    for i in range(num_conv_pools):\n","      final_dim = (final_dim + 0 - self.k) // 1 + 1\n","      final_dim = final_dim // self.pool_size\n","    return final_dim\n","\n","  # Add batch norm and dropout\n","  def forward(self, x):\n","      x = self.conv1(x)\n","      x = F.relu(x) # swap with other activations\n","      x = self.pool(x)\n","      x = self.conv2(x)\n","      x = F.relu(x) # swap with other activations\n","      x = self.pool(x)\n","      x = torch.flatten(x, 1) # flatten all dimensions except batch\n","      x = F.relu(self.fc1(x)) # swap with other activations\n","      x = F.relu(self.fc2(x)) # swap with other activations\n","      x = self.fc3(x)\n","      return x\n","    "],"metadata":{"id":"CxqH8nKuRXi2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["IN_DIMS = 32 \n","IN_CHANNELS = 3\n","N_CLASSES = 10\n","net = BabyThanos(in_dims=IN_DIMS, in_channels=IN_CHANNELS, n_classes=N_CLASSES)"],"metadata":{"id":"O-dwbekbT49S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(net)"],"metadata":{"id":"P4L8u-SMgIrN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Hyperparameters \n","**Note:** Use the best values from the previous exercise"],"metadata":{"id":"bDozysqD6F8c"}},{"cell_type":"code","source":["BATCH_SIZE = 64\n","EPOCHS = 20\n","LEARNING_RATE = 1e-3"],"metadata":{"id":"rf_QykFL6EgS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n","                                         shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"metadata":{"id":"4mvlOIOA76Io"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define Optimizer & Loss Function\n","These two functions allow us to help baby thanos learn from his mistakes."],"metadata":{"id":"KL_5TfglTy3D"}},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss() # Loss Function\n","optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE) # Optimizer "],"metadata":{"id":"Hnj1-mPITxjE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"metadata":{"id":"fpC1vbLuUbHX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train and Evaluate the Network\n","The training and validation loops call the same set of functions over and over, so we'll package them into separate functions. Note that the validation loop does not have any optimizer calls. "],"metadata":{"id":"b13gdPYtUBmF"}},{"cell_type":"code","source":["from tqdm.notebook import tqdm\n","def train_step(model, train_loader, optimizer, criterion):\n","  model.train()\n","  epoch_loss = []\n","  total, correct = 0, 0\n","\n","  for i, batch in tqdm(enumerate(train_loader), \n","                       total=len(train_loader),\n","                       leave=False,\n","                       ):\n","    images, labels = batch\n","    images = images.to(device)\n","    labels = labels.to(device)\n","\n","    optimizer.zero_grad() # Erase history - clean slate\n","\n","    predictions = model(images) # forward -> model (images) -> make guesses on labels\n","    loss = criterion(predictions, labels) # how did I do?\n","    epoch_loss.append(loss.item())\n","    _, predicted = torch.max(predictions.data, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item() # Accuracy score\n","    loss.backward() # backward pass\n","    optimizer.step() # Update the weights using gradients\n","  \n","  return np.mean(epoch_loss), correct / total\n"],"metadata":{"id":"oT7wrZHVT8z9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def valid_step(model, val_loader, criterion):\n","  model.eval()\n","  epoch_loss = []\n","  total, correct = 0, 0\n","\n","  with torch.no_grad():\n","    for i, batch in tqdm(enumerate(val_loader), \n","                        total=len(val_loader),\n","                        leave=False,\n","                        ):\n","      images, labels = batch\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      # Note that there's no optimizer here\n","      predictions = model(images)\n","      loss = criterion(predictions, labels)\n","      epoch_loss.append(loss.item())\n","      _, predicted = torch.max(predictions.data, 1)\n","      total += labels.size(0)\n","      correct += (predicted == labels).sum().item()\n","  \n","  return np.mean(epoch_loss), correct / total"],"metadata":{"id":"2GkxuVRVVm-B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net = net.to(device)\n","losses = {\"train_loss\": [], \"val_loss\": []}\n","accuracies = {\"train_acc\": [], \"val_acc\": []}\n","epochs = []\n","for epoch in tqdm(range(EPOCHS), total=EPOCHS):\n","  train_loss, train_acc = train_step(net, \n","                                     trainloader, \n","                                     optimizer, \n","                                     criterion,)\n","  \n","  val_loss, val_acc = valid_step(net, \n","                                 testloader, \n","                                 criterion,\n","                                 )\n","  \n","  losses[\"train_loss\"].append(train_loss)\n","  losses[\"val_loss\"].append(val_loss)\n","  accuracies[\"train_acc\"].append(train_acc)\n","  accuracies[\"val_acc\"].append(val_acc)\n","  epochs.append(epoch)\n","\n","  print(f'[{epoch + 1}] train loss: {train_loss}  train accuracy: {train_acc}  val loss: {val_loss}  val accuracy: {val_acc}')"],"metadata":{"id":"_bvQQNXYV6yu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Plot the Loss and Accuracy of our Model"],"metadata":{"id":"ym8N-On3AABv"}},{"cell_type":"code","source":["plt.figure(figsize=(8,8))\n","plt.plot(epochs, losses[\"train_loss\"], label=\"train\")\n","plt.plot(epochs, losses[\"val_loss\"], label=\"val\")\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"loss\")\n","plt.grid(\"on\")\n","plt.legend()\n","plt.title(\"Loss vs Epochs\")"],"metadata":{"id":"9S3y9L1L-WCc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8,8))\n","plt.plot(epochs, accuracies[\"train_acc\"], label=\"train\")\n","plt.plot(epochs, accuracies[\"val_acc\"], label=\"val\")\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"accuracy\")\n","plt.grid(\"on\")\n","plt.legend()\n","plt.title(\"Accuracy vs Epochs\")"],"metadata":{"id":"MaZ5ZfTu_TfI"},"execution_count":null,"outputs":[]}]}
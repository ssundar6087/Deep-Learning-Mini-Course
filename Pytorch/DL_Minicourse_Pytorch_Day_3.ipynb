{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"19OqQ7FR1IBbmN1lS-BsCwHKdEV0LcVsa","timestamp":1665203799318},{"file_id":"1MeWh78fXU0GbNwbw2uTJySmfvw1KWjKq","timestamp":1665103902452},{"file_id":"1pwiOBgRVAgW7KGVFdJWdTTamlQ7bfUXe","timestamp":1665103747612}],"collapsed_sections":[],"authorship_tag":"ABX9TyM86v/tamYN49PYcpKds8Us"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["<a href=\"https://colab.research.google.com/github/ssundar6087/Deep-Learning-Mini-Course/blob/main/Pytorch/DL_Minicourse_Pytorch_Day_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"],"metadata":{"id":"PpV8ziEuttsk"}},{"cell_type":"markdown","source":["# Deconstructing the training loop\n","Today, we'll look at the training and validation loops and play with hyperparameters. Note that 90% of the code will be the same as the previous notebook. Our focus is to play with the hyperparameters and see if we can improve results."],"metadata":{"id":"e--Ti2Ov8GTm"}},{"cell_type":"markdown","source":["# Image Classification Pytorch"],"metadata":{"id":"tGV5DlzgRFNK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJmKqClAQ4x9"},"outputs":[],"source":["# Imports\n","import torch \n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"markdown","source":["## Define our Model"],"metadata":{"id":"SMBKXZyURnyE"}},{"cell_type":"code","source":["class BabyThanos(nn.Module):\n","  def __init__(self, in_ftrs, n_classes=10):\n","    super().__init__()\n","    # define the layers here\n","    self.in_ftrs = in_ftrs # Our first neural net will flatten the image to a vector\n","    self.fc1_dim = 256\n","    self.fc2_dim = 128\n","    self.fc1 = nn.Linear(self.in_ftrs, self.fc1_dim)\n","    self.fc2 = nn.Linear(self.fc1_dim, self.fc2_dim)\n","    self.clf_head = nn.Linear(self.fc2_dim, n_classes)\n","  \n","  # define the flow of data through the layers here\n","  def forward(self, x):\n","    x = x.reshape(-1, self.in_ftrs) # Flatten the image\n","    x = self.fc1(x)\n","    x = F.relu(x) # Pass output through activation layer\n","    x = self.fc2(x)\n","    x = F.relu(x) \n","    x = self.clf_head(x) # (batch_size, num_classes)\n","    return x\n","    "],"metadata":{"id":"CxqH8nKuRXi2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["IN_FTRS = 32 * 32 * 3 # CIFAR-10 images are 32 x 32 and have 3 channels\n","net = BabyThanos(in_ftrs=IN_FTRS, n_classes=10)"],"metadata":{"id":"O-dwbekbT49S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(net)"],"metadata":{"id":"P4L8u-SMgIrN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **YOUR EXERCISE HERE: Hyperparameters to tune** ðŸ‘‡\n","While there are a ton of hyperparameters that we can play with, we'll focus on the three most commonly tuned ones:\n","- Epochs : Number of times the model sees the entire dataset\n","- Batch Size: How many examples of the dataset the model sees at a time\n","- Learning Rate: How much the gradients affect the weights (See the optimizer call below)\n","\n","Change these values one at a time and observe how the training curves change. As a suggestion, play with the learning rate first and then try changing the batch size and then the number of epochs. How does the model's performances change?"],"metadata":{"id":"bDozysqD6F8c"}},{"cell_type":"code","source":["# CHANGE THESE VALUES AND SEE WHAT HAPPENS\n","BATCH_SIZE = 64\n","EPOCHS = 20\n","LEARNING_RATE = 1e-3"],"metadata":{"id":"rf_QykFL6EgS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n","                                         shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"metadata":{"id":"4mvlOIOA76Io"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define Optimizer & Loss Function\n","These two functions allow us to help baby thanos learn from his mistakes."],"metadata":{"id":"KL_5TfglTy3D"}},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss() # Loss Function\n","optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE) # Optimizer "],"metadata":{"id":"Hnj1-mPITxjE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"metadata":{"id":"fpC1vbLuUbHX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train and Evaluate the Network\n","The training and validation loops call the same set of functions over and over, so we'll package them into separate functions. Note that the validation loop does not have any optimizer calls. "],"metadata":{"id":"b13gdPYtUBmF"}},{"cell_type":"code","source":["from tqdm.notebook import tqdm\n","def train_step(model, train_loader, optimizer, criterion):\n","  model.train()\n","  epoch_loss = []\n","  total, correct = 0, 0\n","\n","  for i, batch in tqdm(enumerate(train_loader), \n","                       total=len(train_loader),\n","                       leave=False,\n","                       ):\n","    images, labels = batch\n","    images = images.to(device)\n","    labels = labels.to(device)\n","\n","    optimizer.zero_grad() # Erase history - clean slate\n","\n","    predictions = model(images) # forward -> model (images) -> make guesses on labels\n","    loss = criterion(predictions, labels) # how did I do?\n","    epoch_loss.append(loss.item())\n","    _, predicted = torch.max(predictions.data, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item() # Accuracy score\n","    loss.backward() # backward pass\n","    optimizer.step() # Update the weights using gradients\n","  \n","  return np.mean(epoch_loss), correct / total\n"],"metadata":{"id":"oT7wrZHVT8z9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def valid_step(model, val_loader, criterion):\n","  model.eval()\n","  epoch_loss = []\n","  total, correct = 0, 0\n","\n","  with torch.no_grad():\n","    for i, batch in tqdm(enumerate(val_loader), \n","                        total=len(val_loader),\n","                        leave=False,\n","                        ):\n","      images, labels = batch\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      # Note that there's no optimizer here\n","      predictions = model(images)\n","      loss = criterion(predictions, labels)\n","      epoch_loss.append(loss.item())\n","      _, predicted = torch.max(predictions.data, 1)\n","      total += labels.size(0)\n","      correct += (predicted == labels).sum().item()\n","  \n","  return np.mean(epoch_loss), correct / total"],"metadata":{"id":"2GkxuVRVVm-B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net = net.to(device)\n","losses = {\"train_loss\": [], \"val_loss\": []}\n","accuracies = {\"train_acc\": [], \"val_acc\": []}\n","epochs = []\n","for epoch in tqdm(range(EPOCHS), total=EPOCHS):\n","  train_loss, train_acc = train_step(net, \n","                                     trainloader, \n","                                     optimizer, \n","                                     criterion,)\n","  \n","  val_loss, val_acc = valid_step(net, \n","                                 testloader, \n","                                 criterion,\n","                                 )\n","  \n","  losses[\"train_loss\"].append(train_loss)\n","  losses[\"val_loss\"].append(val_loss)\n","  accuracies[\"train_acc\"].append(train_acc)\n","  accuracies[\"val_acc\"].append(val_acc)\n","  epochs.append(epoch)\n","\n","  print(f'[{epoch + 1}] train loss: {train_loss}  train accuracy: {train_acc}  val loss: {val_loss}  val accuracy: {val_acc}')"],"metadata":{"id":"_bvQQNXYV6yu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Plot the Loss and Accuracy of our Model"],"metadata":{"id":"ym8N-On3AABv"}},{"cell_type":"code","source":["plt.figure(figsize=(8,8))\n","plt.plot(epochs, losses[\"train_loss\"], label=\"train\")\n","plt.plot(epochs, losses[\"val_loss\"], label=\"val\")\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"loss\")\n","plt.grid(\"on\")\n","plt.legend()\n","plt.title(\"Loss vs Epochs\")"],"metadata":{"id":"9S3y9L1L-WCc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8,8))\n","plt.plot(epochs, accuracies[\"train_acc\"], label=\"train\")\n","plt.plot(epochs, accuracies[\"val_acc\"], label=\"val\")\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"accuracy\")\n","plt.grid(\"on\")\n","plt.legend()\n","plt.title(\"Accuracy vs Epochs\")"],"metadata":{"id":"MaZ5ZfTu_TfI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9uuajjQNDId1"},"execution_count":null,"outputs":[]}]}
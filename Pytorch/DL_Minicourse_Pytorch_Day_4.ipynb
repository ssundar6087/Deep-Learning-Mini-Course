{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1b52y2OosIWkdyNVqITs-LKCxjkR_1GHM","timestamp":1665205349714},{"file_id":"19OqQ7FR1IBbmN1lS-BsCwHKdEV0LcVsa","timestamp":1665203799318},{"file_id":"1MeWh78fXU0GbNwbw2uTJySmfvw1KWjKq","timestamp":1665103902452},{"file_id":"1pwiOBgRVAgW7KGVFdJWdTTamlQ7bfUXe","timestamp":1665103747612}],"collapsed_sections":[],"authorship_tag":"ABX9TyPkozRPX9bPMpx3nX1uQyEy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["<a href=\"https://colab.research.google.com/github/ssundar6087/Deep-Learning-Mini-Course/blob/main/Pytorch/DL_Minicourse_Pytorch_Day_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"],"metadata":{"id":"zWWXdvRAty-I"}},{"cell_type":"markdown","source":["# Deconstructing the Model Part 1\n","Today, we'll look at the model. Note that 90% of the code will be the same as the previous notebook. Our focus is to play with the model architecture and see if we can improve results."],"metadata":{"id":"e--Ti2Ov8GTm"}},{"cell_type":"markdown","source":["# Image Classification Pytorch"],"metadata":{"id":"tGV5DlzgRFNK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJmKqClAQ4x9"},"outputs":[],"source":["# Imports\n","import torch \n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"markdown","source":["## **YOUR EXERCISE HERE: Define the Model  ðŸ‘‡** \n","In today's exercise, you'll be creating a convolutional neural network. Previously, we flatten a 3 channel image (32 x 32 x 3) into a vector and passed that through linear layers. It's now time to spruce up the model and pass the image directly through convolutional layers.\n","\n","**Convolutions?**\n","The example below is for 2D convolutions, but generally, you have as many dimensions as you have in the input stream (images, audio, time-series, point clouds, etc.). $B$ is the batch size and $C$ is the number of channels. $dim$ is the resolution of the input which in this case is 32. Finally, $k$ is the size of the convolutional kernel.\n","\n","  - What is my input shape: $B \\times C \\times dim \\times dim$ \n","\n","  - What is my output shape: $B \\times C \\times dim' \\times dim'$\n","\n","  - Here $dim' = \\frac{(dim + 2 \\times padding - k)}{stride} + 1$\n","\n","**Potential Pitfall**\n","\n","Watch out for the transition from the convolutional layers to the linear layers. You will have to compute the dimensions of the _flattened_ tensor at the end of convolutions.\n"],"metadata":{"id":"SMBKXZyURnyE"}},{"cell_type":"markdown","source":["### HINT: DO NOT USE UNTIL YOU'VE TRIED IT YOURSELF\n","ðŸ‘‰[Convolutional Networks In Pytorch](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) ðŸ‘ˆ"],"metadata":{"id":"dLSqauDTCgDq"}},{"cell_type":"code","source":["class BabyThanos(nn.Module):\n","  def __init__(self, in_dims, in_channels, n_classes=10):\n","    super().__init__()\n","    \n","    self.in_dims = in_dims \n","    self.in_channels = in_channels\n","    self.n_classes = n_classes\n","    # define the layers here\n","  \n","  \n","  def forward(self, x):\n","    # define the flow of data through the layers here\n","    \n","    return x\n","    "],"metadata":{"id":"CxqH8nKuRXi2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["IN_DIMS = 32 \n","IN_CHANNELS = 3\n","N_CLASSES = 10\n","net = BabyThanos(in_dims=IN_DIMS, in_channels=IN_CHANNELS, n_classes=N_CHANNELS)"],"metadata":{"id":"O-dwbekbT49S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(net)"],"metadata":{"id":"P4L8u-SMgIrN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Hyperparameters \n","**Note:** Use the best values from the previous exercise"],"metadata":{"id":"bDozysqD6F8c"}},{"cell_type":"code","source":["BATCH_SIZE = 64\n","EPOCHS = 20\n","LEARNING_RATE = 1e-3"],"metadata":{"id":"rf_QykFL6EgS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n","                                         shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"metadata":{"id":"4mvlOIOA76Io"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define Optimizer & Loss Function\n","These two functions allow us to help baby thanos learn from his mistakes."],"metadata":{"id":"KL_5TfglTy3D"}},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss() # Loss Function\n","optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE) # Optimizer "],"metadata":{"id":"Hnj1-mPITxjE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"metadata":{"id":"fpC1vbLuUbHX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train and Evaluate the Network\n","The training and validation loops call the same set of functions over and over, so we'll package them into separate functions. Note that the validation loop does not have any optimizer calls. "],"metadata":{"id":"b13gdPYtUBmF"}},{"cell_type":"code","source":["from tqdm.notebook import tqdm\n","def train_step(model, train_loader, optimizer, criterion):\n","  model.train()\n","  epoch_loss = []\n","  total, correct = 0, 0\n","\n","  for i, batch in tqdm(enumerate(train_loader), \n","                       total=len(train_loader),\n","                       leave=False,\n","                       ):\n","    images, labels = batch\n","    images = images.to(device)\n","    labels = labels.to(device)\n","\n","    optimizer.zero_grad() # Erase history - clean slate\n","\n","    predictions = model(images) # forward -> model (images) -> make guesses on labels\n","    loss = criterion(predictions, labels) # how did I do?\n","    epoch_loss.append(loss.item())\n","    _, predicted = torch.max(predictions.data, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item() # Accuracy score\n","    loss.backward() # backward pass\n","    optimizer.step() # Update the weights using gradients\n","  \n","  return np.mean(epoch_loss), correct / total\n"],"metadata":{"id":"oT7wrZHVT8z9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def valid_step(model, val_loader, criterion):\n","  model.eval()\n","  epoch_loss = []\n","  total, correct = 0, 0\n","\n","  with torch.no_grad():\n","    for i, batch in tqdm(enumerate(val_loader), \n","                        total=len(val_loader),\n","                        leave=False,\n","                        ):\n","      images, labels = batch\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      # Note that there's no optimizer here\n","      predictions = model(images)\n","      loss = criterion(predictions, labels)\n","      epoch_loss.append(loss.item())\n","      _, predicted = torch.max(predictions.data, 1)\n","      total += labels.size(0)\n","      correct += (predicted == labels).sum().item()\n","  \n","  return np.mean(epoch_loss), correct / total"],"metadata":{"id":"2GkxuVRVVm-B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net = net.to(device)\n","losses = {\"train_loss\": [], \"val_loss\": []}\n","accuracies = {\"train_acc\": [], \"val_acc\": []}\n","epochs = []\n","for epoch in tqdm(range(EPOCHS), total=EPOCHS):\n","  train_loss, train_acc = train_step(net, \n","                                     trainloader, \n","                                     optimizer, \n","                                     criterion,)\n","  \n","  val_loss, val_acc = valid_step(net, \n","                                 testloader, \n","                                 criterion,\n","                                 )\n","  \n","  losses[\"train_loss\"].append(train_loss)\n","  losses[\"val_loss\"].append(val_loss)\n","  accuracies[\"train_acc\"].append(train_acc)\n","  accuracies[\"val_acc\"].append(val_acc)\n","  epochs.append(epoch)\n","\n","  print(f'[{epoch + 1}] train loss: {train_loss}  train accuracy: {train_acc}  val loss: {val_loss}  val accuracy: {val_acc}')"],"metadata":{"id":"_bvQQNXYV6yu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Plot the Loss and Accuracy of our Model"],"metadata":{"id":"ym8N-On3AABv"}},{"cell_type":"code","source":["plt.figure(figsize=(8,8))\n","plt.plot(epochs, losses[\"train_loss\"], label=\"train\")\n","plt.plot(epochs, losses[\"val_loss\"], label=\"val\")\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"loss\")\n","plt.grid(\"on\")\n","plt.legend()\n","plt.title(\"Loss vs Epochs\")"],"metadata":{"id":"9S3y9L1L-WCc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8,8))\n","plt.plot(epochs, accuracies[\"train_acc\"], label=\"train\")\n","plt.plot(epochs, accuracies[\"val_acc\"], label=\"val\")\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"accuracy\")\n","plt.grid(\"on\")\n","plt.legend()\n","plt.title(\"Accuracy vs Epochs\")"],"metadata":{"id":"MaZ5ZfTu_TfI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9uuajjQNDId1"},"execution_count":null,"outputs":[]}]}
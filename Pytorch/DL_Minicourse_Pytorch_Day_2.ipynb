{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1MeWh78fXU0GbNwbw2uTJySmfvw1KWjKq","timestamp":1665103902452},{"file_id":"1pwiOBgRVAgW7KGVFdJWdTTamlQ7bfUXe","timestamp":1665103747612}],"collapsed_sections":[],"authorship_tag":"ABX9TyPjgug4fTi4ozuuie9U/kdi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["<a href=\"https://colab.research.google.com/github/ssundar6087/Deep-Learning-Mini-Course/blob/main/Pytorch/DL_Minicourse_Pytorch_Day_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"],"metadata":{"id":"VeG4NPQjtl_V"}},{"cell_type":"markdown","source":["# End to End Network\n","Today, we'll be training and validating our first neural net. Let's call it **Baby Thanos**. Over the course of the next few lessons, let's see if **Baby Thanos** can collect all the infinity stones and become inevitable. ðŸ’ \n","\n","Don't worry if none of this makes sense yet. Just run all the cells in the notebook and observe the outputs. We'll look at each piece in isolation to better understand deep learning. Let's start ðŸ˜€"],"metadata":{"id":"e--Ti2Ov8GTm"}},{"cell_type":"markdown","source":["# Image Classification Pytorch"],"metadata":{"id":"tGV5DlzgRFNK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJmKqClAQ4x9"},"outputs":[],"source":["# Imports\n","import torch \n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"markdown","source":["## Load Dataset - CIFAR-10\n","The dataset has 10 classes with 50k training images and 10k test images"],"metadata":{"id":"UF3HfcZbRN2J"}},{"cell_type":"code","source":["transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","BATCH_SIZE = 64\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n","                                         shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"metadata":{"id":"OhzX7u6zRM8w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Show Some Images"],"metadata":{"id":"t_6t6IQz9mB0"}},{"cell_type":"code","source":["def imshow(img):\n","    img = img / 2 + 0.5  \n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","dataiter = iter(trainloader)\n","images, labels = dataiter.next()\n","\n","imshow(torchvision.utils.make_grid(images[:4, ...]))\n","print(' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"],"metadata":{"id":"GdBGdVyJ9Zoy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define our Model"],"metadata":{"id":"SMBKXZyURnyE"}},{"cell_type":"code","source":["class BabyThanos(nn.Module):\n","  def __init__(self, in_ftrs, n_classes=10):\n","    super().__init__()\n","    # define the layers here\n","    self.in_ftrs = in_ftrs # Our first neural net will flatten the image to a vector\n","    self.fc1_dim = 256\n","    self.fc2_dim = 128\n","    self.fc1 = nn.Linear(self.in_ftrs, self.fc1_dim)\n","    self.fc2 = nn.Linear(self.fc1_dim, self.fc2_dim)\n","    self.clf_head = nn.Linear(self.fc2_dim, n_classes)\n","  \n","  # define the flow of data through the layers here\n","  def forward(self, x):\n","    x = x.reshape(-1, self.in_ftrs) # Flatten the image\n","    x = self.fc1(x)\n","    x = F.relu(x) # Pass output through activation layer\n","    x = self.fc2(x)\n","    x = F.relu(x) \n","    x = self.clf_head(x) # (batch_size, num_classes)\n","    return x\n","    "],"metadata":{"id":"CxqH8nKuRXi2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["IN_FTRS = 32 * 32 * 3 # CIFAR-10 images are 32 x 32 and have 3 channels\n","net = BabyThanos(in_ftrs=IN_FTRS, n_classes=10)"],"metadata":{"id":"O-dwbekbT49S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(net)"],"metadata":{"id":"P4L8u-SMgIrN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define Optimizer & Loss Function\n","These two functions allow us to help baby thanos learn from his mistakes."],"metadata":{"id":"KL_5TfglTy3D"}},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss() # Loss Function\n","optimizer = optim.SGD(net.parameters(), lr=0.001) # Optimizer "],"metadata":{"id":"Hnj1-mPITxjE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"metadata":{"id":"fpC1vbLuUbHX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train and Evaluate the Network\n","The training and validation loops call the same set of functions over and over, so we'll package them into separate functions. Note that the validation loop does not have any optimizer calls. "],"metadata":{"id":"b13gdPYtUBmF"}},{"cell_type":"code","source":["from tqdm.notebook import tqdm\n","def train_step(model, train_loader, optimizer, criterion):\n","  model.train()\n","  epoch_loss = []\n","  total, correct = 0, 0\n","\n","  for i, batch in tqdm(enumerate(train_loader), \n","                       total=len(train_loader),\n","                       leave=False,\n","                       ):\n","    images, labels = batch\n","    images = images.to(device)\n","    labels = labels.to(device)\n","\n","    optimizer.zero_grad() # Erase history - clean slate\n","\n","    predictions = model(images) # forward -> model (images) -> make guesses on labels\n","    loss = criterion(predictions, labels) # how did I do?\n","    epoch_loss.append(loss.item())\n","    _, predicted = torch.max(predictions.data, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item() # Accuracy score\n","    loss.backward() # backward pass\n","    optimizer.step() # Update the weights using gradients\n","  \n","  return np.mean(epoch_loss), correct / total\n"],"metadata":{"id":"oT7wrZHVT8z9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def valid_step(model, val_loader, criterion):\n","  model.eval()\n","  epoch_loss = []\n","  total, correct = 0, 0\n","\n","  with torch.no_grad():\n","    for i, batch in tqdm(enumerate(val_loader), \n","                        total=len(val_loader),\n","                        leave=False,\n","                        ):\n","      images, labels = batch\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      # Note that there's no optimizer here\n","      predictions = model(images)\n","      loss = criterion(predictions, labels)\n","      epoch_loss.append(loss.item())\n","      _, predicted = torch.max(predictions.data, 1)\n","      total += labels.size(0)\n","      correct += (predicted == labels).sum().item()\n","  \n","  return np.mean(epoch_loss), correct / total"],"metadata":{"id":"2GkxuVRVVm-B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 20\n","net = net.to(device)\n","losses = {\"train_loss\": [], \"val_loss\": []}\n","accuracies = {\"train_acc\": [], \"val_acc\": []}\n","epochs = []\n","for epoch in tqdm(range(EPOCHS), total=EPOCHS):\n","  train_loss, train_acc = train_step(net, \n","                                     trainloader, \n","                                     optimizer, \n","                                     criterion,)\n","  \n","  val_loss, val_acc = valid_step(net, \n","                                 testloader, \n","                                 criterion,\n","                                 )\n","  \n","  losses[\"train_loss\"].append(train_loss)\n","  losses[\"val_loss\"].append(val_loss)\n","  accuracies[\"train_acc\"].append(train_acc)\n","  accuracies[\"val_acc\"].append(val_acc)\n","  epochs.append(epoch)\n","\n","  print(f'[{epoch + 1}] train loss: {train_loss}  train accuracy: {train_acc}  val loss: {val_loss}  val accuracy: {val_acc}')"],"metadata":{"id":"_bvQQNXYV6yu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Plot the Loss and Accuracy of our Model"],"metadata":{"id":"ym8N-On3AABv"}},{"cell_type":"code","source":["plt.figure(figsize=(8,8))\n","plt.plot(epochs, losses[\"train_loss\"], label=\"train\")\n","plt.plot(epochs, losses[\"val_loss\"], label=\"val\")\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"loss\")\n","plt.grid(\"on\")\n","plt.legend()\n","plt.title(\"Loss vs Epochs\")"],"metadata":{"id":"9S3y9L1L-WCc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8,8))\n","plt.plot(epochs, accuracies[\"train_acc\"], label=\"train\")\n","plt.plot(epochs, accuracies[\"val_acc\"], label=\"val\")\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"accuracy\")\n","plt.grid(\"on\")\n","plt.legend()\n","plt.title(\"Accuracy vs Epochs\")"],"metadata":{"id":"MaZ5ZfTu_TfI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9uuajjQNDId1"},"execution_count":null,"outputs":[]}]}